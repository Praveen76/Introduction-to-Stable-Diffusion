{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/Introduction-to-Stable-Diffusion/blob/main/Introduction_to_StableDiffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the concept of stable diffusion\n",
        "* implement the pre-trained model to generate images from the text prompt\n",
        "* understand the different parameters to generate refined images"
      ],
      "metadata": {
        "id": "H9AHpL1-GxZK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIWZTIQwUPv3"
      },
      "source": [
        "## Stable Diffusion\n",
        "\n",
        "Stable Diffusion is a deep learning model for image generation,\n",
        "released in 2022. It is based on a particular type of diffusion model called `Latent Diffusion`, proposed in the paper [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf). They are generative models, designed to  create new data similar to the data it was trained on. In the case of Stable Diffusion, the data is images and trained to reduce random Gaussian noise.\n",
        "\n",
        "The Stable Diffusion algorithm was developed by Compvis (the Computer Vision research group at the Ludwig Maximilian University of Munich) and sponsored primarily by the startup Stability AI.\n",
        "\n",
        "The algorithm is based on ideas from DALL-E 2 (developed by Open AI, which is also the creator of ChatGPT), Imagen (from Google), and others image generation models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of image generation through diffusion models:\n",
        "\n",
        "* **Unconditional image generation:** the model generates images without any additional condition like text or image. You will get images similar to those provided in the training set.\n",
        "\n",
        "* The **generation of images conditioned** by text is known as `text-to-image`, or `text2img`. The prompt is converted into embeddings that are used to condition the model to\n",
        "generate an image from noise.\n",
        "\n",
        "* The **generation of images based on other image** is known as `image-to-image`, or `img2img`. In addition to the text prompt, it allows sending an initial image to condition the  generation of new images. You have more control over the final composition.\n",
        "\n",
        "* **`Inpainting`** allows selecting a specific part of the image to change  the class/concept, or even  removing it from the scene.\n"
      ],
      "metadata": {
        "id": "P3iVeqFVxjo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main components of Stable diffusion\n",
        "\n",
        "Main components include:\n",
        "\n",
        "1. Autoencoder (VAE)\n",
        "2. U-Net\n",
        "3. Text-encoder\n",
        "4. CLIP (Contrastive Language–Image Pre-training\n",
        "\n",
        "Figure below shows the conceptual diagram of stable diffusion architecture/inference steps:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src = \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Introduction_to_StableDiffusion/stable-diffusion.webp?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T042806Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=ccb1d3a0a3f6763b0b1aa2d4e21c7863ccee13dcb192f1574cf9208460f92f37\" width=700px/>\n",
        "</center>\n",
        "\n",
        "* SD receives a latent seed and a text prompt as input.\n",
        "* The seed is used to generate random representations of\n",
        "latent images of size 64x64.\n",
        "* The text prompt is transformed into text embeddings of\n",
        "size 77x768 using CLIP text encoder.\n",
        "* U-Net iteratively reduces noise from the random latent\n",
        "image representations while conditioning on the text\n",
        "embeddings.\n",
        "* The U-Net output (noise residual) is used to compute a\n",
        "denoised latent image representation using a scheduler\n",
        "algorithm.\n",
        "* The denoising process is repeated $x$ times to recover the\n",
        "best latent image representations.\n",
        "* Once completed, the latent image representation is\n",
        "decoded by the decoder part of the VAE.\n"
      ],
      "metadata": {
        "id": "DZHN18gizF1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Steps:"
      ],
      "metadata": {
        "id": "rKQ0Fvl_jNqU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyleOySVQLiU"
      },
      "source": [
        "## Installing the libraries\n",
        "\n",
        "- [xformers](https://github.com/facebookresearch/xformers) for memory optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers==0.11.1\n",
        "!pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
      ],
      "metadata": {
        "id": "nkRWkNzZ6Eei",
        "outputId": "26a6732d-143e-4d6a-deba-bc6cad3798cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.9/524.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.4/314.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "1j0FSJBgFTYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xfbZWZj2FWHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaF8ebGrL7AV"
      },
      "source": [
        "## Load Pipeline for Image generation\n",
        "\n",
        "- We can define with little effort a pipeline to use the Stable Diffusion model, through the [StableDiffusionPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py).\n",
        "\n",
        "- The checkpoint used here is the [`'CompVis/stable-diffusion-v1-4'`](https://huggingface.co/CompVis/stable-diffusion-v1-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch    #PyTorch\n",
        "import jax\n",
        "jax.random.KeyArray = jax.Array           # since jax.random.KeyArray is deprecated\n",
        "from diffusers import StableDiffusionPipeline"
      ],
      "metadata": {
        "id": "S7_nLNw06vfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)   # float32"
      ],
      "metadata": {
        "id": "39DZnTMG699O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting and using GPU\n",
        "pipe = pipe.to('cuda')"
      ],
      "metadata": {
        "id": "DotxEMaC7q2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory optimization, specially on colab\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "cLXAW8-S7ws7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n3_6qEQNT5S"
      },
      "source": [
        "## Generating the image\n",
        "\n",
        "**Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the prompt\n",
        "\n",
        "prompt = 'a tiger standing on grass land'\n",
        "\n",
        "# eg. 'a photograph of an astronaut riding a horse"
      ],
      "metadata": {
        "id": "Y04r6qOnARpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference from pipeline\n",
        "\n",
        "img = pipe(prompt).images[0]\n",
        "print(type(img), img.size)\n",
        "img"
      ],
      "metadata": {
        "id": "F86f33CoAXEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example2**"
      ],
      "metadata": {
        "id": "IPMvHLJtQM1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'a tiger sitting on tree'\n",
        "img = pipe(prompt).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "bvvTErFTA_fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUrAa9NaMtEA"
      },
      "source": [
        "Saving the result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your image as png file\n",
        "img.save('result.png')"
      ],
      "metadata": {
        "id": "PGteuOI0BVSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs7EwVDbaqd6"
      },
      "source": [
        "## Generating multiple images\n",
        "\n",
        "Provide the value of parameter `num_images_per_prompt` inside pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for plotting multiple images as one\n",
        "\n",
        "def grid_img(imgs, rows=1, cols=3, scale=1):\n",
        "  assert len(imgs) == rows * cols\n",
        "\n",
        "  w, h = imgs[0].size\n",
        "  w, h = int(w * scale), int(h * scale) # reduce the size : when s<1 ; 0.75 * 512 = 384\n",
        "\n",
        "  grid = Image.new('RGB', size = (cols * w, rows * h)) # creating gird (like blank canvas)\n",
        "  grid_w, grid_h = grid.size\n",
        "\n",
        "  for i, img in enumerate(imgs): # looping throuhg images\n",
        "    img = img.resize((w, h), Image.ANTIALIAS) # second parameter preserve the image appearance\n",
        "    grid.paste(img, box=(i % cols * w, i // cols * h)) # pasting the image on grids\n",
        "\n",
        "  return grid"
      ],
      "metadata": {
        "id": "zeCITxUSDV8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example3**"
      ],
      "metadata": {
        "id": "w7RHZudfRBem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_imgs = 3\n",
        "prompt = 'photograph of a royal enfield'\n",
        "imgs = pipe(prompt, num_images_per_prompt= num_imgs).images\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "r3ocqsHyE6aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-59GKURNNpY"
      },
      "source": [
        "## Getting the same image on every run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdF-fVufNwkD"
      },
      "source": [
        "* Provide **`generator` parameter** with fixed value of `seed`.\n",
        "* seed is used to initialize the generation of random numbers. It is used to create the initial latent noise.\n",
        "* By setting seed, we can reproduce the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single image\n",
        "seed = 777\n",
        "generator = torch.Generator('cuda').manual_seed(seed)\n",
        "img = pipe(prompt, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "-ZgNX6ztHIW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple image --> using above defined grid_img() function\n",
        "\n",
        "prompt = \"photograph of a royal enfield'\"\n",
        "seed = 777\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "imgs = pipe(prompt, num_images_per_prompt=num_imgs, generator=generator).images\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "6Jbb5v1pH3Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example4**"
      ],
      "metadata": {
        "id": "6jlueLTCRMjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "imgs = pipe(prompt, num_images_per_prompt=num_imgs, generator=generator).images\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "kW_0BPHwITLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters\n",
        "\n",
        "There are many other parameters like `generator` that can be passed inside the StableDiffusionPipeline to improve the result.\n",
        "\n",
        "For example:\n",
        "- Inference steps (`num_inference_steps`)\n",
        "- Guidance scale (`guidance_scale`)\n",
        "- Image size (`height` and `width` dimensions)\n",
        "- Negative prompt (`negative_prompt`)"
      ],
      "metadata": {
        "id": "B5gcpaisKgVN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY-_5pNuN9W3"
      },
      "source": [
        "#### **Inference steps (`num_inference_steps`)**\n",
        "\n",
        "The more the steps, the better the results but the longer it takes to generate the image. It is also known as denoising steps, as it indicates the number of steps required to turn the image from complete noise (initial state) into the result.\n",
        "\n",
        "* Stable Diffusion works very well with a relatively small number of steps, so we recommend using the default value of 50 inference steps. The more steps the better the result, but there comes a point where  the image stops improving.\n",
        "* For faster results, use a smaller number.\n",
        "* The defaut number of steps varies according to the scheduler algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "img = pipe(prompt, num_inference_steps=20, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "NGC2f82NDnL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating image by passing different inference steps from 10 to 50:"
      ],
      "metadata": {
        "id": "TDLjI3QsdU1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "\n",
        "for i in range(1, 6):\n",
        "    n_steps = i * 10\n",
        "    #print(n_steps)\n",
        "    generator = torch.Generator('cuda').manual_seed(seed)\n",
        "    img = pipe(prompt, num_inference_steps=n_steps, generator=generator).images[0]\n",
        "\n",
        "    plt.subplot(1, 5, i)\n",
        "    plt.title('num_inference_steps: {}'.format(n_steps))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ux1KpwPkD8DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKx-PDG-44Ak"
      },
      "source": [
        "#### **Guidance scale (CFG)**\n",
        "\n",
        "The classifier-free guidance (CFG - also known as guidance scale) is a way to increase the adherence to  the conditional prompt that guides the generation, as well as the overall quality of the image.\n",
        "It controls how much the prompt will be taken into account for conditioning the diffusion process.\n",
        "\n",
        "* **Smaller values:** the more the prompt is ignored. For example, if the value is set to 0 then the image\n",
        "generation is unconditioned.\n",
        "* **Higher values:** returns images that better represent the prompt\n",
        "\n",
        "\n",
        "\n",
        "Choosing the best value\n",
        "\n",
        "* Values between 7 and 8.5 are generally good choices. The default value is 7.5\n",
        "\n",
        "* In general, you can keep the value in the range from 5 to 9. Less realistic images will be returned if the values are too low or too high\n",
        "\n",
        "* The best value depends on the desired results and the complexity of the prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "img = pipe(prompt, guidance_scale=10, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "9ex2KJOhGat_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating image by passing different CFG values from  from 5 to 9:"
      ],
      "metadata": {
        "id": "cQVHMOMjfdI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "\n",
        "for i in range(1, 6):\n",
        "    n_guidance = i + 4\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "    img = pipe(prompt, guidance_scale=n_guidance, generator=generator).images[0]\n",
        "\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.title('guidance_scale: {}'.format(n_guidance))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FMcWrmVdG6IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZW-3-88GPP-"
      },
      "source": [
        "#### **Image size (dimensions)**\n",
        "\n",
        "The generated images are 512 x 512 pixels\n",
        "\n",
        "Recommendations in case you want other dimensions:\n",
        "\n",
        "* make sure the height and width are multiples of 8\n",
        "* less than 512 will result in lower quality images\n",
        "* exceeding 512 in both directions (width and height) will repeat areas of the image (\"global coherence\" is lost)\n",
        "\n",
        "> **Landscape mode**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 777\n",
        "prompt = \"photograph of a mountain landscape during sunset, stars in the sky\"\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "h, w = 512, 768\n",
        "img = pipe(prompt, height=h, width=w, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "Rftrmd9YH2hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LTMjyACUhcd"
      },
      "source": [
        "> **Portrait mode**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "h, w = 768, 512\n",
        "img = pipe(prompt, height=h, width=w, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "Lgg9jy79IXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60KcBA1wabiX"
      },
      "source": [
        "#### **Negative prompt (`negative_prompt`)**\n",
        "\n",
        "The negative prompt is an\n",
        "additional way of telling what you\n",
        "don't want in the image, what\n",
        "you'd like to avoid.\n",
        "* It could be used to remove objects\n",
        "from the image or fix defects.\n",
        "* It is optional in the first versions of\n",
        "Stable Diffusion, however, in the\n",
        "latest versions it has become\n",
        "important to generate\n",
        "quality images.\n",
        "* Some images can only be\n",
        "generated using negative prompts.\n",
        "\n",
        "During the text-to-image conditioning step, the prompt is converted into embedding\n",
        "vectors which will feed the U-Net noise predictor.\n",
        "* There are two sets of embedding vectors: one for the positive prompt and one for the\n",
        "negative prompt.\n",
        "* Both the positive and negative prompts have 77 tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 3\n",
        "\n",
        "prompt = 'photograph of an old car'\n",
        "neg_prompt = 'bw photo'                  # black and white photo\n",
        "\n",
        "imgs = pipe(prompt, negative_prompt = neg_prompt, num_images_per_prompt= num_images).images\n",
        "\n",
        "grid = grid_img(imgs, rows= 1, cols= 3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "VFjXiqs0OfA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R12ZXHxMX7N"
      },
      "source": [
        "## Other models\n",
        "\n",
        "Stable Diffusion models (sometimes called as checkpoint files) are pre-trained  weights used to generate images, for general or specific purposes.\n",
        "* The images that a model can generate depend on the data used to train it. For  example, a model will not be able to generate an image of a cat if there are no  cats in the training data.\n",
        "* If you train a model with images of cats, it should be able to only generate cats.\n",
        "\n",
        "* The official models released by Stability AI and its partners are called base models,  or general-purpose models.\n",
        "\n",
        "\n",
        "Diffusion model that was publicly available:\n",
        "\n",
        "* v1.4 - Released in August 2022 by Stability AI. It is considered the first Stable [https://huggingface.co/CompVis/stable-diffusion-v-1-4-original]\n",
        "\n",
        "* v1.5 - released in October 2022 by Stability AI and Runway\n",
        "ML. It is based on its  predecessor, with additional training.\n",
        "[https://huggingface.co/runwayml/stable-diffusion-v1-5]\n",
        "\n",
        "\n",
        "\n",
        "* v2.0 - Released in November 2022 by Stability AI\n",
        "In addition to 512 × 512 dimensions, a higher resolution version of 768 × 768 pixels is  available.\n",
        "\n",
        "* v2.1 - Released in December 2022 by Stability AI\n",
        "\n",
        "* Differences in v2\n",
        "\n",
        "  Stable Diffusion v1 uses CLIP Open AI for text embedding.\n",
        "\n",
        "  Stable Diffusion v2 uses OpenClip for text embedding.\n",
        "\n",
        "Main reasons for the change: OpenClip is currently five times bigger. A larger text encoder model improves  image quality. It is able to better “understand” the prompt.\n",
        "\n",
        "All are considered general-purpose models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL-mnwxQ5Zmu"
      },
      "source": [
        "## Trying different versions\n",
        "\n",
        "### **SD v1.5**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd15 = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "sd15 = sd15.to('cuda')\n",
        "sd15.enable_attention_slicing()\n",
        "sd15.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "nzUzOgXMTS8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_imgs = 3\n",
        "\n",
        "prompt = \"photograph of an old car\"\n",
        "neg_prompt = \"bw photo\"\n",
        "\n",
        "imgs = sd15(prompt, negative_prompt=neg_prompt, num_images_per_prompt=num_imgs).images\n",
        "\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "SPX5jPJvT9OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"photo of a futuristic city on another planet, realistic, full hd\"\n",
        "neg_prompt = 'buildings'\n",
        "\n",
        "imgs = sd15(prompt, negative_prompt = neg_prompt, num_images_per_prompt=num_imgs).images\n",
        "\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "pw20dYNBUU1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT6GeHs9WT94"
      },
      "source": [
        "### **SD v2.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"#C24641\">\n",
        "NOTE : To run SD v2.1 given below, first *Disconnect and delete the runtime* then connect to it again, otherwise the colab notebook will run out of space.</font>**"
      ],
      "metadata": {
        "id": "Gh66u6Z3N0wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q diffusers==0.11.1\n",
        "!pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n",
        "\n",
        "# Import required packages\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import jax\n",
        "jax.random.KeyArray = jax.Array\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "4-OXTiKnLMAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd2 = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
        "sd2 = sd2.to(\"cuda\")\n",
        "sd2.enable_attention_slicing()\n",
        "sd2.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "hDPtixsDVhvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for plotting multiple images as one\n",
        "\n",
        "def grid_img(imgs, rows=1, cols=3, scale=1):\n",
        "  assert len(imgs) == rows * cols\n",
        "\n",
        "  w, h = imgs[0].size\n",
        "  w, h = int(w * scale), int(h * scale) # reduce the size : when s<1 ; 0.75 * 512 = 384\n",
        "\n",
        "  grid = Image.new('RGB', size = (cols * w, rows * h)) # creating gird (like blank canvas)\n",
        "  grid_w, grid_h = grid.size\n",
        "\n",
        "  for i, img in enumerate(imgs): # looping throuhg images\n",
        "    img = img.resize((w, h), Image.ANTIALIAS) # second parameter preserve the image appearance\n",
        "    grid.paste(img, box=(i % cols * w, i // cols * h)) # pasting the image on grids\n",
        "\n",
        "  return grid"
      ],
      "metadata": {
        "id": "2p8-q95-QL8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_imgs=3\n",
        "prompt = \"photo of a futuristic city on another planet, realistic, full hd\"\n",
        "neg_prompt = 'buildings'\n",
        "\n",
        "imgs = sd2(prompt, negative_prompt=neg_prompt, num_images_per_prompt=num_imgs).images\n",
        "\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "-qejrGBwV34M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amHeOOgVxAmS"
      },
      "source": [
        "### Fine-tuned models with specific styles\n",
        "\n",
        "* **Additional training** – Training a base model with an additional dataset. For example, you can train Stable Diffusion with an additional old car dataset to orient the aesthetics of the  cars to that specific type.\n",
        "\n",
        "* [**Dreambooth**](https://dreambooth.github.io/) – Initially developed by Google, it is a technique for injecting custom  subjects into the models. Due to its architecture, it is possible to achieve great results  using only 3/5 custom images.\n",
        "\n",
        "* **Textual inversion** (also called Embedding) – It injects a custom subject into the model  with just a few examples. A new keyword is created specifically for the new object and  the training is executed only in the embedding neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see some fine tuned models available :\n",
        "\n",
        "* Modern Disney style- https://huggingface.co/nitrosocke/mo-di-diffusion\n",
        "\n",
        "* Classic Disney Style - https://huggingface.co/nitrosocke/classic-anim-diffusion\n",
        "\n",
        "* High resolution 3D animation - https://huggingface.co/nitrosocke/redshift-diffusion\n",
        "\n",
        "* Futuristic images - https://huggingface.co/nitrosocke/Future-Diffusion\n",
        "\n",
        "* Other animation styles:\n",
        " * https://huggingface.co/nitrosocke/Ghibli-Diffusion\n",
        " * https://huggingface.co/nitrosocke/spider-verse-diffusion\n",
        "* More models https://huggingface.co/models?other=stable-diffusion-diffusers\n",
        "\n"
      ],
      "metadata": {
        "id": "0yDTZgljPtk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modern Disney style\n",
        "\n",
        "modi = StableDiffusionPipeline.from_pretrained(\"nitrosocke/mo-di-diffusion\", torch_dtype=torch.float16)\n",
        "modi = modi.to(\"cuda\")\n",
        "modi.enable_attention_slicing()\n",
        "modi.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "W29uykKsdX8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse, modern disney style\"\n",
        "\n",
        "seed = 777\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "imgs = modi(prompt, generator=generator, num_images_per_prompt=num_imgs).images\n",
        "\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "xG-PoMsLd-7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"panda, modern disney style\"\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "imgs = modi(prompt, generator=generator, num_images_per_prompt=3).images\n",
        "\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.5)\n",
        "grid"
      ],
      "metadata": {
        "id": "FjUYu251edcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"albert einstein, modern disney style\",\n",
        "          \"modern disney style rolls-royce in the desert, golden hour\",\n",
        "          \"modern disney style helicopter\"]\n",
        "\n",
        "seed = 42\n",
        "print(\"Seed: \".format(str(seed)))\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "imgs = modi(prompt, generator=generator).images\n",
        "\n",
        "grid = grid_img(imgs, rows=1, cols=3, scale=0.75)\n",
        "grid"
      ],
      "metadata": {
        "id": "cQHg52NPexG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVadq8jiT_ZV"
      },
      "source": [
        "## Changing the Scheduler (sampler)\n",
        "\n",
        "The model is usually not trained to directly predict a slightly less noisy image, but rather to predict the '***noise residual***' which is the difference between a less noisy image and the input image or, similarly, the gradient between the two time steps.\n",
        "\n",
        "To do the denoising process, a specific ***noise scheduling algorithm*** is thus necessary and \"wrap\" the model to define how many diffusion steps are needed for inference as well as how to *compute* a less noisy image from the model's output. Here is where the different ***schedulers*** of the diffusers library come into play.\n",
        "\n",
        "Scheduler algorithms (also called samplers) calculates the predicted denoised image  representation from the previous noise representation and the predicted noise residual. Determines how the image is calculated. There are several different algorithms.\n",
        "\n",
        "Some examples commonly used with Stable Diffusion :\n",
        "\n",
        "* PNDM (default)\n",
        "* DDIM Scheduler\n",
        "* K-LMS Scheduler\n",
        "* Euler Ancestral Discrete Scheduler (Euler A)\n",
        "* DPM Scheduler\n",
        "\n",
        "For technical details on theory and mathematics of the algorithms, refer to [this](https://arxiv.org/abs/2206.00364) paper: Elucidating the Design Space of Diffusion-Based Generative Models.\n",
        "\n",
        "For comparison of different samplers, refer [here](https://www.artstation.com/blogs/kaddoura/pBPo/stable-diffusion-samplers).\n",
        "\n",
        "To know more about available schedulers on HuggingFace, refer [here](https://huggingface.co/docs/diffusers/using-diffusers/schedulers#schedulers-summary).\n",
        "\n",
        "Default is [PNDMScheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying with verion 1.5\n",
        "sd15 = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "sd15 = sd15.to(\"cuda\")\n",
        "sd15.enable_attention_slicing()\n",
        "sd15.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "yQvbR447fmZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the scheduler attributes\n",
        "sd15.scheduler"
      ],
      "metadata": {
        "id": "8m_5KFU2gDgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 555\n",
        "prompt = \"photo of a cute baby girl wearing sunglasses, on the beach, ocean in the background\"\n",
        "generator = torch.Generator('cuda').manual_seed(seed)\n",
        "img = sd15(prompt, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "GCQV2340gOaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting other compatible scheduler with the algorithm\n",
        "sd15.scheduler.compatibles"
      ],
      "metadata": {
        "id": "5qivBAWtgxO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the current config\n",
        "sd15.scheduler.config"
      ],
      "metadata": {
        "id": "UziqUUmAhITz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing the scheduler to `DDIMScheduler`"
      ],
      "metadata": {
        "id": "vjz7F_7DSw_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDIMScheduler\n",
        "sd15.scheduler = DDIMScheduler.from_config(sd15.scheduler.config)"
      ],
      "metadata": {
        "id": "LdC1JSVng7Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator(device = 'cuda').manual_seed(seed)\n",
        "img = sd15(prompt, generator=generator).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "9l7u8ESahOha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing the scheduler to `LMSDiscreteScheduler`"
      ],
      "metadata": {
        "id": "uxTlUzfWZLgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "sd15.scheduler = LMSDiscreteScheduler.from_config(sd15.scheduler.config)\n",
        "\n",
        "generator = torch.Generator(device = 'cuda').manual_seed(seed)\n",
        "img = sd15(prompt, num_inference_steps = 50, generator=generator).images[0]\n",
        "# num_inference_steps = 60, try with different value\n",
        "img"
      ],
      "metadata": {
        "id": "th3RSHTNiGKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing the scheduler to `EulerAncestralDiscreteScheduler`"
      ],
      "metadata": {
        "id": "pLrSrJsrTAFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import EulerAncestralDiscreteScheduler\n",
        "\n",
        "sd15.scheduler = EulerAncestralDiscreteScheduler.from_config(sd15.scheduler.config)\n",
        "\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "img = sd15(prompt, generator=generator, num_inference_steps=60).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "bscni3O8jMYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing the scheduler to `EulerDiscreteScheduler`"
      ],
      "metadata": {
        "id": "sWTrGWCyTO_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import EulerDiscreteScheduler\n",
        "\n",
        "sd15.scheduler = EulerDiscreteScheduler.from_config(sd15.scheduler.config)\n",
        "\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "img = sd15(prompt, generator=generator, num_inference_steps=50).images[0]\n",
        "img"
      ],
      "metadata": {
        "id": "DLuOGYd4jsfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all the above examples, try with negative prompt to improve the results."
      ],
      "metadata": {
        "id": "owWY0YVoXul_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HktYxPKKFsS"
      },
      "source": [
        "### **NOTE**:\n",
        "It is important to check the license of the models, especially if you intend to make some commercial use with the obtained results.\n",
        "\n",
        "1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content,\n",
        "2. We claim no rights on the outputs you generate, you are free to use them and are accountable for their use which should not go against the provisions set in the license, and\n",
        "3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users.\n",
        "```\n",
        "(Licence of v1.4 e v1.5 https://huggingface.co/spaces/CompVis/stable-diffusion-license)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References:\n",
        "\n",
        "1. [How diffusion models work: the math from scratch](https://theaisummer.com/diffusion-models/)\n",
        "\n",
        "2. [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)\n",
        "\n",
        "3. [Stable Diffusion using diffusers](https://www.cnblogs.com/flyingsir/p/17175876.html)\n",
        "\n",
        "4. [Stable Diffusion Paper](https://arxiv.org/abs/2112.10752) - High-Resolution Image Synthesis with Latent Diffusion Models\n",
        "\n",
        "5. [DreamBooth Paper](https://arxiv.org/pdf/2208.12242.pdf) - DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\n",
        "6. [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)\n",
        "7. [Classifier-Free Diffusion Guidance Paper](https://arxiv.org/pdf/2207.12598.pdf)\n",
        "8. [Generating images with Stable Diffusion](https://blog.paperspace.com/generating-images-with-stable-diffusion/)\n",
        "\n",
        "9. [DreamBooth](https://huggingface.co/docs/diffusers/training/dreambooth)\n",
        "10. [Stable Diffusion prompt: a definitive guide](https://stable-diffusion-art.com/prompt-guide/)\n",
        "11. [Text-guided image-to-image generation](https://huggingface.co/docs/diffusers/using-diffusers/img2img)"
      ],
      "metadata": {
        "id": "mfMsUFZyZczx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}